# Ollama Optimizations

## Performance Settings

AIThink sá»­ dá»¥ng cÃ¡c tá»‘i Æ°u hÃ³a sau cho Ollama:

### 1. Flash Attention
- **Variable:** `OLLAMA_FLASH_ATTENTION=1`
- **Lá»£i Ã­ch:** Xá»­ lÃ½ context dÃ i nhanh hÆ¡n vÃ  tiáº¿t kiá»‡m RAM
- **PhÃ¹ há»£p:** Models vá»›i context length lá»›n (>8K tokens)

### 2. KV Cache Quantization
- **Variable:** `OLLAMA_KV_CACHE_TYPE=q8_0`
- **Lá»£i Ã­ch:** Giáº£m 50-60% dung lÆ°á»£ng bá»™ nhá»› cho KV cache
- **PhÃ¹ há»£p:** Há»™i thoáº¡i dÃ i, nhiá»u turns

## Usage

### Start Ollama vá»›i optimizations:
```bash
./start-ollama.sh
```

### Check optimizations:
```bash
./status.sh
```

Output sáº½ hiá»ƒn thá»‹:
```
ðŸ”¹ Ollama (port 11434):
   âœ… Ollama is running (version: 0.14.3)
   âš¡ Flash Attention: enabled
   ðŸ’¾ KV Cache: q8_0 (optimized)
```

## Requirements

- Ollama version: **0.14.3** or higher
- Models: Há»— trá»£ táº¥t cáº£ models, Ä‘áº·c biá»‡t hiá»‡u quáº£ vá»›i:
  - deepseek-r1:8b (thinking model)
  - Models vá»›i context length > 32K

## Performance Impact

### Memory Usage:
- **Without optimization:** ~2GB RAM cho 32K context
- **With optimization:** ~800MB-1GB RAM

### Speed:
- **Flash Attention:** +30-40% faster vá»›i long context
- **KV Cache q8_0:** Minimal speed impact (~5% slower)

## Notes

- CÃ¡c optimizations Ä‘Æ°á»£c Ã¡p dá»¥ng khi start Ollama
- Cáº§n restart Ollama Ä‘á»ƒ thay Ä‘á»•i cÃ³ hiá»‡u lá»±c
- Backend AIThink khÃ´ng cáº§n restart khi thay Ä‘á»•i Ollama settings
